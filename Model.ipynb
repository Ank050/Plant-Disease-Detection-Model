{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f014ade8",
   "metadata": {},
   "source": [
    "# **Potato Disease Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6105ab64",
   "metadata": {},
   "source": [
    "## Import Modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27ff1545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3312a5a",
   "metadata": {},
   "source": [
    "\n",
    "* tensorflow is imported as tf.\n",
    "* The models and layers sub-modules are imported from tensorflow.keras. These sub-modules provide functions for creating and training neural network models, as well as building and configuring different types of layers.\n",
    "* matplotlib.pyplot is imported as plt, which allows you to create different types of plots and visualizations to analyze your data.\n",
    "* IPython.display.HTML is imported to display HTML content in the Jupyter notebook environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dc3329",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f071a0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256\n",
    "CHANNELS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1c5dff",
   "metadata": {},
   "source": [
    "## Data Argumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "481c5019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32917 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=10,\n",
    "        horizontal_flip=True\n",
    ")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'dataset/train',\n",
    "        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "        batch_size=32,\n",
    "        class_mode=\"sparse\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aff553",
   "metadata": {},
   "source": [
    "* This code uses the ImageDataGenerator class from tensorflow.keras.preprocessing.image module to perform data augmentation on image data during training of a deep learning model. Data augmentation helps to artificially increase the size of the dataset by creating new, slightly modified versions of the existing images, which helps to improve the robustness of the model and reduce overfitting.\n",
    "\n",
    "* The train_datagen object is created with several parameters to specify the types of image augmentations to apply, such as rotating the image by a random angle between -10 and 10 degrees, and randomly flipping the image horizontally. The rescale parameter is used to normalize the pixel values in the image to be between 0 and 1.\n",
    "\n",
    "* The train_generator object is then created using the flow_from_directory() method, which takes the path to the training directory, the target size of the images (IMAGE_SIZE x IMAGE_SIZE), the batch size, and the class mode (sparse in this case, which means that the labels are integers). The generator will read images from the directory, apply the specified augmentations, and generate batches of augmented images for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b8172c",
   "metadata": {},
   "source": [
    "## Class Names with Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8011f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Apple___Black_rot': 0,\n",
       " 'Apple___Cedar_apple_rust': 1,\n",
       " 'Apple___healthy': 2,\n",
       " 'Corn_(maize)___Common_rust_': 3,\n",
       " 'Corn_(maize)___Northern_Leaf_Blight': 4,\n",
       " 'Corn_(maize)___healthy': 5,\n",
       " 'Grape___Black_rot': 6,\n",
       " 'Grape___Esca_(Black_Measles)': 7,\n",
       " 'Grape___healthy': 8,\n",
       " 'Potato___Early_blight': 9,\n",
       " 'Potato___Late_blight': 10,\n",
       " 'Potato___healthy': 11,\n",
       " 'Tomato___Bacterial_spot': 12,\n",
       " 'Tomato___Early_blight': 13,\n",
       " 'Tomato___Late_blight': 14,\n",
       " 'Tomato___healthy': 15}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f2cd85",
   "metadata": {},
   "source": [
    "## Class Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48b61f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple___Black_rot',\n",
       " 'Apple___Cedar_apple_rust',\n",
       " 'Apple___healthy',\n",
       " 'Corn_(maize)___Common_rust_',\n",
       " 'Corn_(maize)___Northern_Leaf_Blight',\n",
       " 'Corn_(maize)___healthy',\n",
       " 'Grape___Black_rot',\n",
       " 'Grape___Esca_(Black_Measles)',\n",
       " 'Grape___healthy',\n",
       " 'Potato___Early_blight',\n",
       " 'Potato___Late_blight',\n",
       " 'Potato___healthy',\n",
       " 'Tomato___Bacterial_spot',\n",
       " 'Tomato___Early_blight',\n",
       " 'Tomato___Late_blight',\n",
       " 'Tomato___healthy']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = list(train_generator.class_indices.keys())\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c98f22f",
   "metadata": {},
   "source": [
    "## Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e159071b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 523 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=10,\n",
    "        horizontal_flip=True)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        'dataset/val',\n",
    "        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "        batch_size=32,\n",
    "        class_mode=\"sparse\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4a7c9d",
   "metadata": {},
   "source": [
    "* This code creates a validation generator similar to the train_generator, but for the validation set. The validation_datagen object is created with the same set of parameters as train_datagen for consistency.\n",
    "\n",
    "* The validation_generator is created using the flow_from_directory() method, which takes the path to the validation directory, the target size of the images (IMAGE_SIZE x IMAGE_SIZE), the batch size, and the class mode (sparse in this case, which means that the labels are integers). The generator will read images from the directory and generate batches of images for validation during the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eae32c",
   "metadata": {},
   "source": [
    "## Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1df606de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1073 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=10,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        'dataset/test',\n",
    "        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "        batch_size=32,\n",
    "        class_mode=\"sparse\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fa68d0",
   "metadata": {},
   "source": [
    "* This code creates a test generator similar to the train_generator and validation_generator. The test_datagen object is created with the same set of parameters as train_datagen and validation_datagen for consistency.\n",
    "\n",
    "* The test_generator is created using the flow_from_directory() method, which takes the path to the test directory, the target size of the images (IMAGE_SIZE x IMAGE_SIZE), the batch size, and the class mode (sparse in this case, which means that the labels are integers). The generator will read images from the directory and generate batches of images for testing the model after training is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae35dd7",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39a7c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "n_classes = 16\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.InputLayer(input_shape=input_shape),\n",
    "    layers.Conv2D(32, kernel_size = (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(n_classes, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa21f0d",
   "metadata": {},
   "source": [
    "* This code defines a convolutional neural network (CNN) model using the Sequential API from tensorflow.keras. The model architecture consists of several convolutional layers, max pooling layers, and dense layers.\n",
    "\n",
    "* The input_shape variable specifies the shape of the input tensor to the model, which is (IMAGE_SIZE, IMAGE_SIZE, CHANNELS), where IMAGE_SIZE is the target size of the input images and CHANNELS is the number of color channels in the images (3 for RGB images).\n",
    "\n",
    "* The n_classes variable specifies the number of classes in the dataset.\n",
    "\n",
    "* The model architecture consists of six convolutional layers, each followed by a max pooling layer to downsample the feature maps. The first convolutional layer has 32 filters with a 3x3 kernel size and uses the relu activation function. The remaining convolutional layers have 64 filters with a 3x3 kernel size and also use the relu activation function.\n",
    "\n",
    "* After the convolutional layers, the feature maps are flattened into a 1D array and passed through two dense layers. The first dense layer has 64 units with a relu activation function, and the second dense layer has n_classes units with a softmax activation function, which outputs a probability distribution over the classes.\n",
    "\n",
    "* This model can be used for image classification tasks on datasets with n_classes number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65b7d44",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b3d8156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 254, 254, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 127, 127, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 125, 125, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 62, 62, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 60, 60, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 30, 30, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 28, 28, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 12, 12, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 6, 6, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 4, 4, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 2, 2, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                1040      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 184,592\n",
      "Trainable params: 184,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc9b80e",
   "metadata": {},
   "source": [
    "## Model Compile with Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f31a5727",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9160e9a0",
   "metadata": {},
   "source": [
    "### This code compiles the CNN model defined in the previous code block using the compile() method.\n",
    "\n",
    "* The optimizer parameter is set to 'adam', which is an optimization algorithm that is commonly used for deep learning models.\n",
    "\n",
    "* The loss parameter is set to tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), which is the loss function used to measure the difference between the predicted and actual labels. SparseCategoricalCrossentropy is used because the labels are in integer form, and not one-hot encoded.\n",
    "\n",
    "* The metrics parameter is set to 'accuracy', which specifies that the accuracy of the model will be used as the evaluation metric during training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5dceef",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e76ae58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 936s 934ms/step - loss: 1.2437 - accuracy: 0.5835 - val_loss: 0.6031 - val_accuracy: 0.8066\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 929s 929ms/step - loss: 0.4224 - accuracy: 0.8545 - val_loss: 0.3572 - val_accuracy: 0.8730\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 931s 930ms/step - loss: 0.2826 - accuracy: 0.9009 - val_loss: 0.2991 - val_accuracy: 0.8906\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 933s 933ms/step - loss: 0.1996 - accuracy: 0.9301 - val_loss: 0.1979 - val_accuracy: 0.9336\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 931s 931ms/step - loss: 0.1628 - accuracy: 0.9417 - val_loss: 0.1476 - val_accuracy: 0.9492\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 928s 928ms/step - loss: 0.1309 - accuracy: 0.9566 - val_loss: 0.3305 - val_accuracy: 0.8770\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 925s 925ms/step - loss: 0.1152 - accuracy: 0.9609 - val_loss: 0.1154 - val_accuracy: 0.9570\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 926s 926ms/step - loss: 0.0946 - accuracy: 0.9678 - val_loss: 0.1192 - val_accuracy: 0.9727\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 925s 924ms/step - loss: 0.0917 - accuracy: 0.9691 - val_loss: 0.0782 - val_accuracy: 0.9785\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 926s 926ms/step - loss: 0.0758 - accuracy: 0.9753 - val_loss: 0.1112 - val_accuracy: 0.9590\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 924s 923ms/step - loss: 0.0701 - accuracy: 0.9761 - val_loss: 0.1456 - val_accuracy: 0.9590\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 925s 925ms/step - loss: 0.0677 - accuracy: 0.9781 - val_loss: 0.0868 - val_accuracy: 0.9727\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 924s 924ms/step - loss: 0.0671 - accuracy: 0.9787 - val_loss: 0.1136 - val_accuracy: 0.9707\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 926s 926ms/step - loss: 0.0484 - accuracy: 0.9837 - val_loss: 0.0822 - val_accuracy: 0.9766\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 926s 926ms/step - loss: 0.0588 - accuracy: 0.9806 - val_loss: 0.0688 - val_accuracy: 0.9805\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 927s 927ms/step - loss: 0.0442 - accuracy: 0.9861 - val_loss: 0.0413 - val_accuracy: 0.9805\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 925s 924ms/step - loss: 0.0569 - accuracy: 0.9817 - val_loss: 0.0536 - val_accuracy: 0.9805\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 929s 928ms/step - loss: 0.0430 - accuracy: 0.9857 - val_loss: 0.0781 - val_accuracy: 0.9766\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 926s 926ms/step - loss: 0.0456 - accuracy: 0.9849 - val_loss: 0.1135 - val_accuracy: 0.9688\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 925s 925ms/step - loss: 0.0447 - accuracy: 0.9858 - val_loss: 0.2532 - val_accuracy: 0.9316\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=1000,\n",
    "    batch_size=32,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=16,\n",
    "    verbose=1,\n",
    "    epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1054361",
   "metadata": {},
   "source": [
    "### This code trains the CNN model on the training set using the fit() method.\n",
    "\n",
    "* The train_generator and validation_generator variables are the generators that generate batches of training and validation data respectively. These generators use data augmentation techniques such as rotation and horizontal flip to increase the amount of training data and prevent overfitting.\n",
    "\n",
    "* The steps_per_epoch parameter is set to 115, which is the number of batches of samples in one epoch of training data. The batch_size parameter is set to 32, which is the number of samples in each batch.\n",
    "\n",
    "* The validation_steps parameter is set to 16, which is the number of batches of samples in one epoch of validation data.\n",
    "\n",
    "* The verbose parameter is set to 1, which specifies the verbosity mode.\n",
    "\n",
    "* The epochs parameter is set to 20, which is the number of times the model will be trained on the entire training dataset.\n",
    "\n",
    "* The training progress and evaluation metrics are stored in the history variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27387c64",
   "metadata": {},
   "source": [
    "## Accuracy of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33fc858b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 14s 398ms/step - loss: 0.1979 - accuracy: 0.9422\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2a6913",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23e66f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"final_more_img.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dfade2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
